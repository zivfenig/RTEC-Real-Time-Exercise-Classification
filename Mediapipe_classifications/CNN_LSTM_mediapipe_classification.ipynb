{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-18T09:39:10.533339Z",
     "start_time": "2025-05-18T09:38:47.060437Z"
    }
   },
   "source": [
    "import cv2\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import pickle\n",
    "from collections import deque\n",
    "from CNN_LSTM_classification import CNN_LSTM "
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T09:39:10.645519Z",
     "start_time": "2025-05-18T09:39:10.543548Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === Load pretrained CNN-LSTM model ===\n",
    "with open(\"model_config.pkl\", \"rb\") as f:\n",
    "    model_config = pickle.load(f)\n",
    "with open(\"label_encoder.pkl\", \"rb\") as f:\n",
    "    label_encoder = pickle.load(f)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CNN_LSTM(**model_config)\n",
    "model.load_state_dict(torch.load(\"cnn_lstm_model.pth\", map_location=device))\n",
    "model.to(device)\n",
    "model.eval()"
   ],
   "id": "6df98641a5f97fb9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN_LSTM(\n",
       "  (conv1): Conv1d(10, 64, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "  (relu): ReLU()\n",
       "  (lstm): LSTM(64, 64, num_layers=2, batch_first=True, dropout=0.3)\n",
       "  (fc): Linear(in_features=64, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T09:39:10.987703Z",
     "start_time": "2025-05-18T09:39:10.983714Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load MediaPipe Pose landmark recognition\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils\n"
   ],
   "id": "918b4cfbc5c0a2a8",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T09:39:11.016352Z",
     "start_time": "2025-05-18T09:39:11.007610Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Calculate angles between points a->b->c \n",
    "def calculate_angle(a, b, c):\n",
    "    a, b, c = np.array(a), np.array(b), np.array(c)\n",
    "    radians = np.arctan2(c[1]-b[1], c[0]-b[0]) - np.arctan2(a[1]-b[1], a[0]-b[0])\n",
    "    angle = np.abs(radians * 180.0 / np.pi)\n",
    "    return 360 - angle if angle > 180 else angle"
   ],
   "id": "a5abfcfd0a1991a9",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T09:39:11.040648Z",
     "start_time": "2025-05-18T09:39:11.034635Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Calculate ground angles\n",
    "def calculate_ground_angle(pt1, pt2):\n",
    "    dx, dy = pt2[0] - pt1[0], pt2[1] - pt1[1]\n",
    "    angle = np.arctan2(dy, dx) * 180.0 / np.pi\n",
    "    return abs(angle)  # angle to horizontal"
   ],
   "id": "df9ff63907819c5d",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T09:39:11.065084Z",
     "start_time": "2025-05-18T09:39:11.058892Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === App config ===\n",
    "SEQUENCE_LENGTH = 120\n",
    "FPS = 40\n",
    "COOLDOWN = 5\n",
    "FEATURE_NAMES = [\n",
    "    \"Shoulder\", \"Elbow\", \"Hip\", \"Knee\", \"Ankle\",\n",
    "    \"Shoulder_Ground\", \"Elbow_Ground\", \"Hip_Ground\", \"Knee_Ground\", \"Ankle_Ground\"\n",
    "]"
   ],
   "id": "4d461dd1ba87ceeb",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T09:39:11.089396Z",
     "start_time": "2025-05-18T09:39:11.082168Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === Smoothing buffers ===\n",
    "# using moving avg to smooth the data\n",
    "angle_buffers = {name: deque(maxlen=5) for name in FEATURE_NAMES}\n",
    "sequence = []\n",
    "collecting = True\n",
    "predicted_label = \"\"\n",
    "start_time = time.time()\n",
    "last_collected = 0"
   ],
   "id": "2f989c1f2036a47c",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T09:48:17.436712Z",
     "start_time": "2025-05-18T09:39:11.108202Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize webcam instance and pose model\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "\n",
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read() # Grab frame from the webcam\n",
    "        if not ret:\n",
    "            break\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) # Convert into RGB format\n",
    "        image.flags.writeable = False\n",
    "        results = pose.process(image) # Processes the image to get the landmark\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        h, w, _ = image.shape\n",
    "\n",
    "        current_time = time.time()\n",
    "\n",
    "        if results.pose_landmarks:\n",
    "            mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS) # Draw the landmark on the screen\n",
    "            lm = results.pose_landmarks.landmark\n",
    "\n",
    "            def vis(i): return lm[i].visibility > 0.0\n",
    "            def pt(i): return [int(lm[i].x * w), int(lm[i].y * h)]\n",
    "\n",
    "            # Validate visibility of all keypoints\n",
    "            required = [\n",
    "                mp_pose.PoseLandmark.LEFT_SHOULDER,\n",
    "                mp_pose.PoseLandmark.LEFT_ELBOW,\n",
    "                mp_pose.PoseLandmark.LEFT_WRIST,\n",
    "                mp_pose.PoseLandmark.LEFT_HIP,\n",
    "                mp_pose.PoseLandmark.LEFT_KNEE,\n",
    "                mp_pose.PoseLandmark.LEFT_ANKLE,\n",
    "                mp_pose.PoseLandmark.LEFT_FOOT_INDEX\n",
    "            ]\n",
    "            if all(vis(p.value) for p in required): # Validate the Body angles actually extract to confirm the vector into the sequence matrix\n",
    "                ls, le, lw = pt(mp_pose.PoseLandmark.LEFT_SHOULDER.value), pt(mp_pose.PoseLandmark.LEFT_ELBOW.value), pt(mp_pose.PoseLandmark.LEFT_WRIST.value)\n",
    "                lh, lk = pt(mp_pose.PoseLandmark.LEFT_HIP.value), pt(mp_pose.PoseLandmark.LEFT_KNEE.value)\n",
    "                la, lf = pt(mp_pose.PoseLandmark.LEFT_ANKLE.value), pt(mp_pose.PoseLandmark.LEFT_FOOT_INDEX.value)\n",
    "\n",
    "                angles = {\n",
    "                \"Shoulder\": calculate_angle(le, ls, lh),\n",
    "                \"Elbow\": calculate_angle(ls, le, lw),\n",
    "                \"Hip\": calculate_angle(ls, lh, lk),\n",
    "                \"Knee\": calculate_angle(lh, lk, la),\n",
    "                \"Ankle\": calculate_angle(lk, la, lf),\n",
    "            \n",
    "                # Ground Angles (e.g., Shoulder to Elbow, Elbow to Wrist, etc.)\n",
    "                \"Shoulder_Ground\": calculate_ground_angle(ls, le),\n",
    "                \"Elbow_Ground\": calculate_ground_angle(le, lw),\n",
    "                \"Hip_Ground\": calculate_ground_angle(lh, lk),\n",
    "                \"Knee_Ground\": calculate_ground_angle(lk, la),\n",
    "                \"Ankle_Ground\": calculate_ground_angle(la, lf)\n",
    "                }\n",
    "\n",
    "                # Smooth angles\n",
    "                smoothed = []\n",
    "                for k in FEATURE_NAMES:\n",
    "                    angle_buffers[k].append(angles[k])\n",
    "                    smoothed.append(np.mean(angle_buffers[k])) # Add the vector to the 120 sequence after smoothed\n",
    "\n",
    "                if collecting and current_time - last_collected >= 1 / FPS:\n",
    "                    if len(sequence) < SEQUENCE_LENGTH:\n",
    "                        sequence.append(smoothed)\n",
    "                        last_collected = current_time\n",
    "\n",
    "                # Draw angle values\n",
    "                for i, name in enumerate(FEATURE_NAMES):\n",
    "                    cv2.putText(image, f\"{name}: {int(smoothed[i])}\", (20, 40 + i * 30),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n",
    "\n",
    "        # === Classification ===\n",
    "        if collecting:\n",
    "            text1 = f\"Collecting: {len(sequence)}/{SEQUENCE_LENGTH}\"\n",
    "            text_size1 = cv2.getTextSize(text1, cv2.FONT_HERSHEY_SIMPLEX, 1.0, 3)[0]\n",
    "            cv2.putText(image, text1, (w - text_size1[0] - 20, h - 80),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 255, 255), 3)\n",
    "            if len(sequence) == SEQUENCE_LENGTH:\n",
    "                X = np.array(sequence).reshape(1, SEQUENCE_LENGTH, 10)\n",
    "                X_tensor = torch.tensor(X, dtype=torch.float32).to(device)\n",
    "                with torch.no_grad(): # Add the collected sequence into the classification model (LSTM+CNN)\n",
    "                    logits = model(X_tensor)\n",
    "                    pred_idx = torch.argmax(logits, dim=1).item()\n",
    "                    predicted_label = label_encoder.inverse_transform([pred_idx])[0]\n",
    "                collecting = False\n",
    "                sequence = []\n",
    "                start_time = time.time()\n",
    "        else:\n",
    "            cooldown = COOLDOWN - (current_time - start_time) # Cooldown for 5 sec before building new sequence\n",
    "            text2 = f\"Prediction: {predicted_label}\"\n",
    "            text3 = f\"Cooldown: {int(max(0, cooldown))}s\"\n",
    "\n",
    "            text_size2 = cv2.getTextSize(text2, cv2.FONT_HERSHEY_SIMPLEX, 1.1, 3)[0]\n",
    "            text_size3 = cv2.getTextSize(text3, cv2.FONT_HERSHEY_SIMPLEX, 0.9, 2)[0]\n",
    "        \n",
    "            cv2.putText(image, text2, (w - text_size2[0] - 20, h - 80),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1.1, (0, 255, 0), 3)\n",
    "            cv2.putText(image, text3, (w - text_size3[0] - 20, h - 40),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 255), 2)\n",
    "\n",
    "            if cooldown <= 0:\n",
    "                collecting = True\n",
    "                last_collected = time.time()\n",
    "\n",
    "        cv2.imshow(\"CNN-LSTM Exercise Classifier\", image)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ],
   "id": "1597c2133fe5e70c",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[8]\u001B[39m\u001B[32m, line 12\u001B[39m\n\u001B[32m     10\u001B[39m image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n\u001B[32m     11\u001B[39m image.flags.writeable = \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m12\u001B[39m results = \u001B[43mpose\u001B[49m\u001B[43m.\u001B[49m\u001B[43mprocess\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimage\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     13\u001B[39m image.flags.writeable = \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[32m     14\u001B[39m image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\envs\\pythonProject\\Lib\\site-packages\\mediapipe\\python\\solutions\\pose.py:185\u001B[39m, in \u001B[36mPose.process\u001B[39m\u001B[34m(self, image)\u001B[39m\n\u001B[32m    164\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mprocess\u001B[39m(\u001B[38;5;28mself\u001B[39m, image: np.ndarray) -> NamedTuple:\n\u001B[32m    165\u001B[39m \u001B[38;5;250m  \u001B[39m\u001B[33;03m\"\"\"Processes an RGB image and returns the pose landmarks on the most prominent person detected.\u001B[39;00m\n\u001B[32m    166\u001B[39m \n\u001B[32m    167\u001B[39m \u001B[33;03m  Args:\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    182\u001B[39m \u001B[33;03m         \"enable_segmentation\" is set to true.\u001B[39;00m\n\u001B[32m    183\u001B[39m \u001B[33;03m  \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m185\u001B[39m   results = \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mprocess\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_data\u001B[49m\u001B[43m=\u001B[49m\u001B[43m{\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mimage\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mimage\u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    186\u001B[39m   \u001B[38;5;28;01mif\u001B[39;00m results.pose_landmarks:  \u001B[38;5;66;03m# pytype: disable=attribute-error\u001B[39;00m\n\u001B[32m    187\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m landmark \u001B[38;5;129;01min\u001B[39;00m results.pose_landmarks.landmark:  \u001B[38;5;66;03m# pytype: disable=attribute-error\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\envs\\pythonProject\\Lib\\site-packages\\mediapipe\\python\\solution_base.py:340\u001B[39m, in \u001B[36mSolutionBase.process\u001B[39m\u001B[34m(self, input_data)\u001B[39m\n\u001B[32m    334\u001B[39m   \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    335\u001B[39m     \u001B[38;5;28mself\u001B[39m._graph.add_packet_to_input_stream(\n\u001B[32m    336\u001B[39m         stream=stream_name,\n\u001B[32m    337\u001B[39m         packet=\u001B[38;5;28mself\u001B[39m._make_packet(input_stream_type,\n\u001B[32m    338\u001B[39m                                  data).at(\u001B[38;5;28mself\u001B[39m._simulated_timestamp))\n\u001B[32m--> \u001B[39m\u001B[32m340\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_graph\u001B[49m\u001B[43m.\u001B[49m\u001B[43mwait_until_idle\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    341\u001B[39m \u001B[38;5;66;03m# Create a NamedTuple object where the field names are mapping to the graph\u001B[39;00m\n\u001B[32m    342\u001B[39m \u001B[38;5;66;03m# output stream names.\u001B[39;00m\n\u001B[32m    343\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._output_stream_type_info \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "7f8048ffd3a9ffe"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
